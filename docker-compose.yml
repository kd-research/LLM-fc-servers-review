name: llama-cpp-python-jupyter

services:
  llama-cpp:
    build: 
      context: .
      dockerfile: ./llama-cpp-python.cuda.dockerfile
    user: "${UID}:${GID}"
    ports:
      - "8000"
    command: 
      - python3 
      - -m 
      - llama_cpp.server
      - --model
      - /models/${MODELSPEC}
      - --hf_pretrained_model_name_or_path
      - /tokenizers
      - --chat_format
      - functionary-v1
      - --n_gpu_layers
      - "-1"
    networks:
      internal_network:
        aliases:
          - llm
    volumes:
      - ${MODELPATH}:/models
      - ${TOKENIZERPATH}:/tokenizers
    env_file:
      - .env
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              driver: nvidia
              count: 1

  jupyter:
    build: 
      context: .
      dockerfile: ./jupyter-lab.dockerfile
    ports:
      - "8888:8888"
    networks:
      - internal_network
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./configs/jupyter_notebook_config.py:/home/jovyan/.jupyter/jupyter_notebook_config.py
    env_file:
      - .env

networks:
  internal_network:


