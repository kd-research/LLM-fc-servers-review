llama-cpp-python supports function calling in a limited way. It has a specific function calling protocol that does not universal applicable.
On the other hand, llama 3 instruct with function calling works when loaded by code. No pipeline is currently setup to be used in server.
